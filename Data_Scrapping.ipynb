{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Scrapping Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import Counter \n",
    "from selenium import webdriver\n",
    "import time\n",
    "from datetime import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining date ranges to collect data for each season\n",
    "<b><u>Note:</u></b> Creating a master array of pf.date_range to loop over to collect data from different websites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dateRanges = []\n",
    "\n",
    "dates2019 = pd.date_range(start=\"2019-04-13\",end=\"2019-10-20\")\n",
    "dateRanges.append(dates2019)\n",
    "dates2018 = pd.date_range(start=\"2018-03-30\",end=\"2018-10-28\")\n",
    "dateRanges.append(dates2018)\n",
    "dates2017 = pd.date_range(start=\"2017-04-02\",end=\"2017-11-01\")\n",
    "dateRanges.append(dates2017)\n",
    "dates2016 = pd.date_range(start=\"2016-04-03\",end=\"2016-11-02\")\n",
    "dateRanges.append(dates2016)\n",
    "dates2015 = pd.date_range(start=\"2015-04-04\",end=\"2015-11-01\")\n",
    "dateRanges.append(dates2015)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Scrapping for Savant Website"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to get extract the table from the wepage\n",
    "<b><u>Note:</u></b> \n",
    "- The source code contains the player's inning by inning data in a table format\n",
    "- Some players had empty cells  for certain feilds that were replaced with NA\n",
    "- Some feilds like BA with svgs that served as an identifier in filling in NA or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTableFromPage(table, headerRow):\n",
    "    headerColumns = headerRow.find_all(\"th\")\n",
    "    bodyBlocks = table.find(\"tbody\")\n",
    "    bodyRows = bodyBlocks.find_all(\"tr\")\n",
    "        \n",
    "    table_contents = []   \n",
    "    header_cells=[]\n",
    "    for th in headerColumns:\n",
    "        cleanedText = th.getText().strip()\n",
    "        if  cleanedText != '':\n",
    "            header_cells.append(cleanedText)\n",
    "    if(len(header_cells) != 0):\n",
    "        table_contents += [header_cells]\n",
    "        \n",
    "        \n",
    "    for tr in bodyRows:\n",
    "        row_cells=[] #make empty list to append cells in the row to\n",
    "    \n",
    "        for td in tr.find_all('td'): # find all the cells in the row that are labled as data cells\n",
    "            cleanedColumnValue = td.getText().strip()\n",
    "            if cleanedColumnValue != '': #remove leading and trailing characters and then check if the string is empty\n",
    "                row_cells.append(cleanedColumnValue)\n",
    "            elif td.find(\"svg\") == None and td.find(\"img\") == None:\n",
    "                row_cells.append(\"NA\")\n",
    "        \n",
    "        if len(row_cells) != 0: \n",
    "            table_contents += [ row_cells ] # if the length of the row_cells is larger than one append it to the tottal list\n",
    "\n",
    "    df = pd.DataFrame(table_contents[1:],columns=table_contents[0])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to get all same day game url from the source code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getURLList (dateRange):\n",
    "    url = \"https://baseballsavant.mlb.com/gamefeed?game_pk=567511&game_date\" + str(dateRange.date())\n",
    "    browser = webdriver.Chrome(r\"C:\\Users\\sarit\\Downloads\\chromedriver_win32\\chromedriver\", chrome_options=options)\n",
    "\n",
    "    browser.get(url)\n",
    "    time.sleep(3)\n",
    "    html = browser.page_source\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    urlDiv = soup.find(\"div\", {\"class\": \"flex highlight-game\"})\n",
    "    getAllGames =soup.find(\"div\", {\"class\": \"flex-container\"})\n",
    "    gameUrlsList = []\n",
    "    if getAllGames != None:\n",
    "        gameLinks = getAllGames.find_all(\"a\")\n",
    "        for link in gameLinks:\n",
    "            gameUrlsList.append(\"https://baseballsavant.mlb.com\" + link[\"href\"])\n",
    "\n",
    "    browser.close()\n",
    "    browser.quit()\n",
    "    return gameUrlsList\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to create the csv file for each game on each day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCSVForDateUrl (url, dateVal, number):\n",
    "    browser = webdriver.Chrome(r\"C:\\Users\\sarit\\Downloads\\chromedriver_win32\\chromedriver\", chrome_options=options)\n",
    "\n",
    "    browser.get(url)\n",
    "    time.sleep(8)\n",
    "    html = browser.page_source\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "    div = soup.find(\"div\", {\"id\": \"exitVelocity\"})\n",
    "    table  = div.find(\"table\")\n",
    "    header = table.find(\"thead\")\n",
    "    headerRow = header.find(\"tr\", attrs={\"class\":\"tr-component-row\"})\n",
    "\n",
    "    df = getTableFromPage(table, headerRow)\n",
    "\n",
    "    browser.close()\n",
    "    browser.quit()\n",
    "   \n",
    "    \n",
    "    newpath = r'C:\\Users\\Public\\Savant\\%s' %str(dateVal.date())\n",
    "    if not os.path.exists(newpath):\n",
    "        os.makedirs(newpath)\n",
    "    \n",
    "    uniqueId = str(dateVal.date()) +\"_\"+ str(number)\n",
    "    filePath = newpath + r'\\Savant_%s.csv' %uniqueId\n",
    "    df.to_csv(filePath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code to loop over evey day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-98-aa4dfc04db2f>:3: DeprecationWarning: use options instead of chrome_options\n",
      "  browser = webdriver.Chrome(r\"C:\\Users\\sarit\\Downloads\\chromedriver_win32\\chromedriver\", chrome_options=options)\n",
      "<ipython-input-99-f109d9d62cd8>:2: DeprecationWarning: use options instead of chrome_options\n",
      "  browser = webdriver.Chrome(r\"C:\\Users\\sarit\\Downloads\\chromedriver_win32\\chromedriver\", chrome_options=options)\n"
     ]
    }
   ],
   "source": [
    "exceptionArray = []\n",
    "\n",
    "for dateRange in dateRanges:\n",
    "    for d in dateRange:\n",
    "        urlList = getURLList(d)\n",
    "        if len(urlList) !=0:\n",
    "            for url in urlList:\n",
    "                try:\n",
    "                    getCSVForDateUrl(url, d, urlList.index(url))                 \n",
    "                except Exception as ex:\n",
    "                    exceptionArray.append(str(ex)+ \"    Url: \" + url + \"  for date: \" + str(d))\n",
    "\n",
    "dfTest = pd.DataFrame(exceptionArray)\n",
    "dfTest.to_csv(r'C:\\Users\\Public\\Savant\\SavantError.csv')    \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Scrapping for Baseball-Reference Website"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting All Links for Batter Box Office Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetAllBoxOfficeLinksForYear(year):\n",
    "    mlbStatsURL = 'https://www.baseball-reference.com/leagues/MLB/%s-schedule.shtml'%year\n",
    "    page = requests.get(mlbStatsURL)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    table = soup.find_all(\"em\")\n",
    "    contentList = []\n",
    "    for link in table:\n",
    "        if link.find(\"a\"):\n",
    "            contentList.append(\"https://www.baseball-reference.com\" + link.find(\"a\")[\"href\"])\n",
    "\n",
    "    return contentList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Pitcher ERA Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = [\"2015\", \"2016\", \"2017\", \"2018\", \"2019\"]\n",
    "for y in years:\n",
    "    exceptionPitcherEraArray = []\n",
    "    urlList = GetAllBoxOfficeLinksForYear(y)\n",
    "       \n",
    "    if len(urlList) !=0:\n",
    "        pitcherERA = []\n",
    "        for url in urlList:\n",
    "            try:\n",
    "                pitcherERA.append(GetCSVForYearData(url)) \n",
    "                \n",
    "            except Exception as ex:\n",
    "                print(ex)\n",
    "                print(url)\n",
    "                print(y)\n",
    "                exceptionPitcherEraArray.append(str(ex) + \" Url: \" + url + \"  for year: \" + y)\n",
    "            \n",
    "        newpath = r'C:\\Users\\Public\\PitcherBoxOffice'\n",
    "        if not os.path.exists(newpath):\n",
    "            os.makedirs(newpath)\n",
    "            \n",
    "        masterDf = pd.concat(pitcherERA)\n",
    "        filePath = newpath + r'\\PictherERA_%s.csv' %y\n",
    "        masterDf.to_csv(filePath)\n",
    "\n",
    "        dfExceptBO = pd.DataFrame(exceptionPitcherEraArray)\n",
    "        dfExceptBO.to_csv(r'C:\\Users\\Public\\PitcherBoxOffice\\PitcherEraError_%s.csv' %y)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPitcherEraDataBoxOfficePage(table, date):\n",
    "    \n",
    "    tableContents = []   \n",
    "\n",
    "    headerRow = table.find(\"thead\")  \n",
    "    headerCells=[]\n",
    "    headerCells.append(headerRow.find(\"th\", attrs={\"data-stat\":\"player\" }).getText())\n",
    "    headerCells.append(headerRow.find(\"th\", attrs={\"data-stat\":\"earned_run_avg\" }).getText())\n",
    "    headerCells.append(\"Date\")\n",
    "\n",
    "\n",
    "    if(len(headerCells) != 0):\n",
    "        tableContents += [headerCells]\n",
    "        \n",
    "        \n",
    "    bodyBlocks = table.find(\"tbody\")\n",
    "    bodyRows = bodyBlocks.find_all(\"tr\")\n",
    "    rowCells=[]\n",
    "    rowCells.append(bodyRows[0].find(\"th\").find(\"a\").getText())\n",
    "    rowCells.append(bodyRows[0].find(\"td\", attrs={\"data-stat\":\"earned_run_avg\"}).getText())\n",
    "    if len(rowCells) != 0: \n",
    "        rowCells.append(date)\n",
    "        tableContents += [ rowCells ] # if the length of the row_cells is larger than one append it to the tottal list\n",
    "\n",
    "    df = pd.DataFrame(tableContents[1:],columns=tableContents[0])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetCSVForYearData(url):\n",
    "    browser = webdriver.Chrome(r\"C:\\Users\\sarit\\Downloads\\chromedriver_win32\\chromedriver\", chrome_options=options)\n",
    "    browser.get(url)\n",
    "    time.sleep(8)\n",
    "    html = browser.page_source\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    tablesList = soup.find_all(\"table\", attrs={\"class\": \"sortable stats_table min_width shade_zero now_sortable sticky_table re1 le1\"})\n",
    "\n",
    "    tablesList.pop(0)\n",
    "    tablesList.pop(0)\n",
    "\n",
    "    picherTableList = []\n",
    "    dateString = re.findall(r'\\d+',url)\n",
    "    dateStringCleaned = dateString[0][:-1]\n",
    "    \n",
    "    date = datetime.strptime(dateStringCleaned, '%Y%m%d')\n",
    "\n",
    "    for table in tablesList:\n",
    "        picherTableList.append(getPitcherEraDataBoxOfficePage(table, str(date.date())))\n",
    "    \n",
    "    browser.close()\n",
    "    browser.quit()\n",
    "        \n",
    "    return pd.concat(picherTableList)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = [\"2015\", \"2016\", \"2017\", \"2018\", \"2019\"]\n",
    "for y in years:\n",
    "    exceptionPitcherEraArray = []\n",
    "    urlList = GetAllBoxOfficeLinksForYear(y)\n",
    "       \n",
    "    if len(urlList) !=0:\n",
    "        pitcherERA = []\n",
    "        for url in urlList:\n",
    "            try:\n",
    "                pitcherERA.append(GetCSVForYearData(url)) \n",
    "                \n",
    "            except Exception as ex:\n",
    "                print(ex)\n",
    "                print(url)\n",
    "                print(y)\n",
    "                exceptionPitcherEraArray.append(str(ex) + \" Url: \" + url + \"  for year: \" + y)\n",
    "            \n",
    "        newpath = r'C:\\Users\\Public\\PitcherBoxOffice'\n",
    "        if not os.path.exists(newpath):\n",
    "            os.makedirs(newpath)\n",
    "            \n",
    "        masterDf = pd.concat(pitcherERA)\n",
    "        filePath = newpath + r'\\PictherERA_%s.csv' %y\n",
    "        masterDf.to_csv(filePath)\n",
    "\n",
    "        dfExceptBO = pd.DataFrame(exceptionPitcherEraArray)\n",
    "        dfExceptBO.to_csv(r'C:\\Users\\Public\\PitcherBoxOffice\\PitcherEraError_%s.csv' %y)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Box Office Data for Batter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTableForBoxOfficePage(table):\n",
    "    teamName = table.find(\"caption\").getText()\n",
    "    header = table.find(\"thead\")\n",
    "    headerRow = header.find(\"tr\")\n",
    "    headerColumns = headerRow.find_all(\"th\")\n",
    "    bodyBlocks = table.find(\"tbody\")\n",
    "    bodyRows = bodyBlocks.find_all(\"tr\")\n",
    "        \n",
    "    table_contents = []   \n",
    "    header_cells=[]\n",
    "    for th in headerColumns:\n",
    "        cleanedText = th.getText().strip()\n",
    "        if  cleanedText != '':\n",
    "            header_cells.append(cleanedText)\n",
    "    header_cells.append(\"Team\")\n",
    "    if(len(header_cells) != 0):\n",
    "        table_contents += [header_cells]\n",
    "        \n",
    "        \n",
    "    for tr in bodyRows:\n",
    "        row_cells=[] #make empty list to append cells in the row to\n",
    "        \n",
    "        if tr.find('th').getText(): #find all cells that are labled as headers in the rows 1 to n and chee\n",
    "            row_cells.append(tr.find('th').find(\"a\").getText()) # append any text in these header cells to the list\n",
    "    \n",
    "    \n",
    "        for td in tr.find_all('td'): # find all the cells in the row that are labled as data cells\n",
    "            cleanedColumnValue = td.getText().strip()\n",
    "            if cleanedColumnValue != '': #remove leading and trailing characters and then check if the string is empty\n",
    "                row_cells.append(cleanedColumnValue)\n",
    "            elif td.find(\"svg\") == None and td.find(\"img\") == None:\n",
    "                row_cells.append(\"NA\")\n",
    "        \n",
    "        if len(row_cells) != 0: \n",
    "            row_cells.append(teamName)\n",
    "            table_contents += [ row_cells ] # if the length of the row_cells is larger than one append it to the tottal list\n",
    "\n",
    "    df = pd.DataFrame(table_contents[1:],columns=table_contents[0])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetCSVForMLBBoxOfficeGame(url, year):\n",
    "    browser = webdriver.Chrome(r\"C:\\Users\\sarit\\Downloads\\chromedriver_win32\\chromedriver\", chrome_options=options)\n",
    "    browser.get(url)\n",
    "    time.sleep(8)\n",
    "    html = browser.page_source\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    uniqueId = soup.find(\"h1\").getText()\n",
    "    tablesList = soup.find_all(\"table\", attrs = {\"class\": \"sortable stats_table min_width shade_zero now_sortable sticky_table re1 le1\"})\n",
    "    listt = []\n",
    "    for table in tablesList:\n",
    "        listt.append(getTableForBoxOfficePage(table))\n",
    "    \n",
    "    df = pd.concat([listt[0],listt[1]])\n",
    "    \n",
    "    browser.close()\n",
    "    browser.quit()\n",
    "        \n",
    "    \n",
    "    newpath = r'C:\\Users\\Public\\BoxOffice\\%s' %year\n",
    "    if not os.path.exists(newpath):\n",
    "        os.makedirs(newpath)\n",
    "    \n",
    "    \n",
    "    filePath = newpath + r'\\BoxOffice_%s.csv' %uniqueId\n",
    "    df.to_csv(filePath)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCSVForEachDate(dateVal, url, year):\n",
    "    browser = webdriver.Chrome(r\"C:\\Users\\sarit\\Downloads\\chromedriver_win32\\chromedriver\", chrome_options=options)\n",
    "    browser.get(url)\n",
    "    time.sleep(8)\n",
    "    html = browser.page_source\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    \n",
    "    \n",
    "    gamesDivList = soup.find_all(\"div\", attrs={\"class\":\"starting-lineups__matchup\"})\n",
    "    gamesDivListDraftKing = soup.find_all(\"div\", attrs={\"class\":\"gamesDivList\"})\n",
    "    gameList = gamesDivList + gamesDivListDraftKing\n",
    "    if len(gameList) == 0:\n",
    "        print(\"nolen\" + dateVal)\n",
    "        return None\n",
    "\n",
    "    dfList = []\n",
    "    for gameDiv in gameList:\n",
    "        df = getTablePitcherBatterMatchUpForGame(gameDiv, dateVal)\n",
    "        dfList.append(df)\n",
    "    \n",
    "    masterdf = pd.concat(dfList)\n",
    "    browser.close()\n",
    "    browser.quit()\n",
    "    \n",
    "    newpath = r'C:\\Users\\Public\\MatchUp\\%s' %year\n",
    "    if not os.path.exists(newpath):\n",
    "        os.makedirs(newpath)\n",
    "    \n",
    "    \n",
    "    filePath = newpath + r'\\PitcherBatterMatchUp_%s.csv' %dateVal\n",
    "    masterdf.to_csv(filePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exceptionBoxOfficeArray = []\n",
    "years = [\"2015\", \"2016\", \"2017\", \"2018\", \"2019\"]\n",
    "years = [\"2015\"]\n",
    "\n",
    "for y in years:\n",
    "    urlList = GetAllBoxOfficeLinksForYear(y)\n",
    "    i = 0\n",
    "    while i< 316:\n",
    "        urlList.pop(0)\n",
    "        i+=1\n",
    "    \n",
    "    if len(urlList) !=0:\n",
    "        for url in urlList:\n",
    "            try:\n",
    "                GetCSVForMLBBoxOfficeGame(url, y)  \n",
    "            except Exception as ex:\n",
    "                exceptionBoxOfficeArray.append(str(ex) + \" Url: \" + url + \"  for year: \" + y)\n",
    "\n",
    "    dfExceptBO = pd.DataFrame(exceptionBoxOfficeArray)\n",
    "    dfExceptBO.to_csv(r'C:\\Users\\Public\\BoxOffice\\BoxOfficeError_%s.csv', y)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Scrapping for SwishAnalytics Website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTablePitcherBatterMatchUpForGameV2(div, dateVal):\n",
    "   \n",
    "    teamNamesInSingleString = div.find(\"h4\" , attrs={ \"class\": \"lato inline vert-mid bold\"}).getText()\n",
    "    teamNames = re.findall(r'\\w+', teamNamesInSingleString) \n",
    "    \n",
    "   \n",
    "    awayPitcherDiv = div.find(\"div\" , attrs={ \"class\": \"mar-right-10\"}).getText()\n",
    "    homePictherDiv = div.find(\"div\" , attrs={ \"class\": \"mar-left-10\"}).getText()\n",
    "     \n",
    "    \n",
    "    awayPitcherInfo = awayPitcherDiv.split(')')\n",
    "    PitcherNameAway = awayPitcherInfo[1].strip()\n",
    "    PitcherHandAway = re.sub('[()]', '', awayPitcherInfo[0])\n",
    "    \n",
    "    homePitcherInfo = homePictherDiv.split('(')\n",
    "    PitcherNameHome = homePitcherInfo[0].strip()\n",
    "    PitcherHandHome = re.sub('[()]', '', homePitcherInfo[1])\n",
    "     \n",
    "          \n",
    "\n",
    "    StartingLineUps = div.find_all(\"tbody\" , attrs={\"class\":\"helvetica\"})\n",
    "   \n",
    "        \n",
    "    table_contents = []   \n",
    "    header_cells=[\"Team\" , \"LineUpPosition\", \"Batter\", \"BatterHand\", \"PitcherFromTheOtherTeam\", \"PitcherHand\", \"Date\", \"Home/Away\",\"Standium\",\"TeamMatchUp\"]\n",
    "    table_contents += [header_cells]\n",
    "    \n",
    "    \n",
    "    battersDivAway = div.find(\"table\", attrs={\"table table-condensed text-left mar-top-0 mar-bottom-0 pad-bottom-0 table-roster\"})\n",
    "    battersListAway = battersDivAway.find_all(\"tr\")\n",
    "        \n",
    "    batterDivHome = div.find(\"table\", attrs={\"class\": \"table table-condensed text-right mar-top-0 mar-bottom-0 pad-bottom-0 table-roster\"})\n",
    "    battersListHome = batterDivHome.find_all(\"tr\")\n",
    "    awayBatterLineUpPosition=1\n",
    "    for batter in battersListAway:\n",
    "        row_cells=[] \n",
    "        row_cells.append(teamNames[0])\n",
    "        row_cells.append(awayBatterLineUpPosition)\n",
    "        row_cells.append(re.sub(r\"[^. a-zA-Z0-9]+\", ' ', batter.find(\"b\").next_sibling))\n",
    "        row_cells.append(re.sub(r\"[^a-zA-Z0-9]+\", ' ',batter.find(\"small\").getText().strip()))\n",
    "        row_cells.append(PitcherNameHome)\n",
    "        row_cells.append(PitcherHandHome)\n",
    "        row_cells.append(dateVal)\n",
    "        row_cells.append(\"Away\")\n",
    "        row_cells.append(teamNames[1])\n",
    "        row_cells.append(re.sub(r\"[^@ a-zA-Z0-9]+\", ' ', teamNamesInSingleString))\n",
    "            #print(row_cells)\n",
    "        if len(row_cells) != 0: \n",
    "            table_contents += [ row_cells ]\n",
    "        awayBatterLineUpPosition+=1\n",
    "        \n",
    "    homeBatterLineUpPosition=1  \n",
    "    for batter in battersListHome:\n",
    "        row_cells=[] \n",
    "        row_cells.append(teamNames[1])\n",
    "        row_cells.append(homeBatterLineUpPosition)\n",
    "        mutedTextTag = batter.select(\"small[class=text-muted]\")\n",
    "        if len(mutedTextTag) == 1:\n",
    "            row_cells.append(re.sub(r\"[^. a-zA-Z0-9]+\", ' ', mutedTextTag[0].next_sibling))\n",
    "            row_cells.append(re.sub(r\"[^a-zA-Z0-9]+\", ' ', mutedTextTag[0].getText().strip()))\n",
    "        else:\n",
    "            row_cells.append(re.sub(r\"[^. a-zA-Z0-9]+\", ' ', mutedTextTag[1].next_sibling))\n",
    "            row_cells.append(re.sub(r\"[^a-zA-Z0-9]+\", ' ',mutedTextTag[1].getText().strip()))\n",
    "        \n",
    "        row_cells.append(PitcherNameAway)\n",
    "        row_cells.append(PitcherHandAway)\n",
    "        row_cells.append(dateVal)\n",
    "        row_cells.append(\"Home\")\n",
    "        row_cells.append(teamNames[1])\n",
    "        row_cells.append(re.sub(r\"[^@ a-zA-Z0-9]+\", ' ', teamNamesInSingleString))\n",
    "            #print(row_cells)\n",
    "        if len(row_cells) != 0: \n",
    "            table_contents += [ row_cells ]\n",
    "        homeBatterLineUpPosition+=1\n",
    "            \n",
    "\n",
    "              \n",
    "        \n",
    "\n",
    "    df = pd.DataFrame(table_contents[1:],columns=table_contents[0])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCSVForEachDateV2(dateVal, url, year):\n",
    "    browser = webdriver.Chrome(r\"C:\\Users\\sarit\\Downloads\\chromedriver_win32\\chromedriver\", chrome_options=options)\n",
    "    browser.get(url)\n",
    "    time.sleep(8)\n",
    "    html = browser.page_source\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    gameDivs = soup.find_all(\"div\", attrs = {\"class\": \"col-md-6\"})\n",
    "    \n",
    "    if len(gameDivs) == 0:\n",
    "        print(\"nolen\" + dateVal)\n",
    "        return None\n",
    "\n",
    "    dfList = []\n",
    "    for div in gameDivs:\n",
    "        df = getTablePitcherBatterMatchUpForGameV2(div, dateVal)\n",
    "        dfList.append(df)\n",
    "    \n",
    "    masterdf = pd.concat(dfList)\n",
    "    browser.close()\n",
    "    browser.quit()\n",
    "    \n",
    "    newpath = r'C:\\Users\\Public\\MatchUp\\%s' %year\n",
    "    if not os.path.exists(newpath):\n",
    "        os.makedirs(newpath)\n",
    "    \n",
    "    \n",
    "    filePath = newpath + r'\\PitcherBatterMatchUp_%s.csv' %dateVal\n",
    "    #filePath = r'C:\\Users\\Public\\MatchUp\\test.csv'\n",
    "    masterdf.to_csv(filePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-b1c37ff48797>:2: DeprecationWarning: use options instead of chrome_options\n",
      "  browser = webdriver.Chrome(r\"C:\\Users\\sarit\\Downloads\\chromedriver_win32\\chromedriver\", chrome_options=options)\n"
     ]
    }
   ],
   "source": [
    "exceptionArray = []\n",
    "\n",
    "for dateRange in dateRanges:\n",
    "    for d in dateRange:\n",
    "        url = \"https://swishanalytics.com/optimus/mlb/lineups?date=\" + str(d.date())\n",
    "        try:\n",
    "            getCSVForEachDateV2(str(d.date()),url, str(d.year))              \n",
    "        except Exception as ex:\n",
    "            print(str(ex) + url)\n",
    "            exceptionArray.append(str(ex)+ \"    Url: \" + url + \"  for date: \" + str(d.date()))\n",
    "\n",
    "dfTest = pd.DataFrame(exceptionArray)\n",
    "errorFilePath =r'C:\\Users\\Public\\MatchUp\\MatchUpError_%s.csv' %str(d.year)\n",
    "dfTest.to_csv(errorFilePath)    \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Scrapping for Baseball-reference website (Only Player Birthday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetCSVForMLBBirthday(url, month, day):\n",
    "    browser = webdriver.Chrome(r\"C:\\Users\\sarit\\Downloads\\chromedriver_win32\\chromedriver\", chrome_options=options)\n",
    "    browser.get(url)\n",
    "    time.sleep(13)\n",
    "    html = browser.page_source\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    table = soup.find(\"table\", attrs = {\"id\":\"birthday_stats\"})\n",
    "\n",
    "    df = getMLBPlayerBirthdayTable(table)\n",
    "        \n",
    "    browser.close()\n",
    "    browser.quit()\n",
    "        \n",
    "    \n",
    "    newpath = r'C:\\Users\\Public\\Birthday\\%s' %month\n",
    "    if not os.path.exists(newpath):\n",
    "        os.makedirs(newpath)\n",
    "        \n",
    "    date= month+\"-\"+day\n",
    "    filePath = newpath + r'\\BirthdayOn_%s.csv' %date\n",
    "    df.to_csv(filePath)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMLBPlayerBirthdayTable(table):\n",
    "    header = table.find(\"thead\")\n",
    "    headerRow = header.find(\"tr\")\n",
    "    bodyBlocks = table.find(\"tbody\")\n",
    "    bodyRows = bodyBlocks.find_all(\"tr\")\n",
    "        \n",
    "    table_contents = []   \n",
    "    header_cells=[]\n",
    "    header_cells.append(headerRow.find(\"th\", attrs={\"data-stat\":\"player\"}).getText().strip())\n",
    "    header_cells.append(headerRow.find(\"th\", attrs={\"data-stat\":\"birth_year\"}).getText().strip())\n",
    "    header_cells.append(headerRow.find(\"th\", attrs={\"data-stat\":\"experience\"}).getText().strip())\n",
    "    header_cells.append(headerRow.find(\"th\", attrs={\"data-stat\":\"year_min\"}).getText().strip())\n",
    "    header_cells.append(headerRow.find(\"th\", attrs={\"data-stat\":\"year_max\"}).getText().strip())\n",
    "                            \n",
    "    if(len(header_cells) != 0):\n",
    "        table_contents += [header_cells]\n",
    "        \n",
    "        \n",
    "    for tr in bodyRows:\n",
    "        row_cells=[] #make empty list to append cells in the row to\n",
    "        lastyear = tr.find(\"td\", attrs={\"data-stat\":\"year_max\"})\n",
    "        if(lastyear == None):\n",
    "            continue\n",
    "        if (int(lastyear.getText().strip()) < 2015):\n",
    "            continue\n",
    "        row_cells.append(tr.find(\"td\", attrs={\"data-stat\":\"player\"}).getText().strip())\n",
    "        row_cells.append(tr.find(\"td\", attrs={\"data-stat\":\"birth_year\"}).getText().strip())\n",
    "        row_cells.append(tr.find(\"td\", attrs={\"data-stat\":\"experience\"}).getText().strip())\n",
    "        row_cells.append(tr.find(\"td\", attrs={\"data-stat\":\"year_min\"}).getText().strip())\n",
    "        row_cells.append(tr.find(\"td\", attrs={\"data-stat\":\"year_max\"}).getText().strip())\n",
    "                \n",
    "        if len(row_cells) != 0: \n",
    "            table_contents += [ row_cells ] # if the length of the row_cells is larger than one append it to the tottal list\n",
    "\n",
    "    df = pd.DataFrame(table_contents[1:],columns=table_contents[0])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exceptionArray = []\n",
    "months = list(map(str,range(1,12)))\n",
    "daysOfYear = pd.date_range(start=\"2020-01-1\",end=\"2020-12-31\")\n",
    "\n",
    "for day in daysOfYear:\n",
    "    url = \"https://www.baseball-reference.com/friv/birthdays.cgi?month=\" + str(day.month) + \"&day=\" + str(day.day)\n",
    "    try:\n",
    "        GetCSVForMLBBirthday(url, str(day.month), str(day.day))\n",
    "    except Exception as ex:\n",
    "        print(str(ex) + url)\n",
    "        exceptionArray.append(str(ex)+ \"    Url: \" + url + \"  for date: \" + str(day))\n",
    "        \n",
    "dfError = pd.DataFrame(exceptionArray)\n",
    "errorFilePath =r'C:\\Users\\Public\\Birthday\\Birhdays.csv'\n",
    "dfError.to_csv(errorFilePath)    \n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
